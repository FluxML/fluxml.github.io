<!DOCTYPE html> <html lang=en > <style> #cppn{ position:absolute; top:0; bottom:0; height: 100%; width: 100vw; } iframe { display: block; border-style:none; } </style> <meta property="og:title" content=Flux.jl > <meta property="og:description" content="The elegant machine learning library"> <meta property="og:image" content="/assets/images/FluxGitHubPreview.png"> <meta property="og:url" content=fluxml.ai > <meta name="twitter:title" content=Flux.jl > <meta name="twitter:description" content="The elegant machine learning library"> <meta name="twitter:image" content="/assets/images/FluxGitHubPreview.png"> <meta name="twitter:card" content=summary_large_image > <link rel=apple-touch-icon  sizes=180x180  href="assets/favicon_io/apple-touch-icon.png"> <link rel=icon  type="image/png" sizes=32x32  href="assets/favicon_io/favicon-32x32.png"> <link rel=icon  type="image/png" sizes=16x16  href="assets/favicon_io/favicon-16x16.png"> <link rel=manifest  href="assets/favicon_io/site.webmanifest"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-36890222-9'); </script> <meta charset=utf-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel=stylesheet  href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin=anonymous > <link rel=stylesheet  href="../../css/script_default.css"> <link rel=stylesheet  href="../../css/site.css"> <link rel=stylesheet  href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin=anonymous > <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin=anonymous > <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin=anonymous ></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin=anonymous  onload="renderMathInElement(document.body, { delimiters: [ {left: '$[[', right: ']]', display: true}, {left: '\\[', right: '\\]', display: true}, {left: '[[', right: ']]', display: false} ] });"> </script> <title>Flux – Elegant ML</title> <nav class="navbar navbar-expand-lg navbar-dark container lighter"> <a class=navbar-brand  href="/./"> <div class=logo  style="font-size:30pt;margin-top:-15px;margin-bottom:-10px;">flux</div> </a> <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class=nav-item > <a class=nav-link  href="https://fluxml.ai/Flux.jl/" target=_blank >Documentation</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/model-zoo/" target=_blank >Model Zoo</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl" target=_blank >GitHub</a> <li class=nav-item > <a class=nav-link  href="https://fluxml.ai/Flux.jl/stable/ecosystem/">Ecosystem</a> <!-- <li class=nav-item > <a class=nav-link  href="/./gsod/">GSoD</a> --> <li class=nav-item > <a class=nav-link  href="/./blog/">Blog</a> <li class=nav-item > <a class=nav-link  href="/./gsoc/">GSoC</a> <li> <a class=nav-link  href="/./governance/">Governance</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl/blob/master/CONTRIBUTING.md" target=_blank >Contribute</a> </ul> </div> </nav> <div class=content > <div class=container > <h1>Accelerating Flux.jl with PyTorch kernels</h1> <div class=franklin-content > <p>Julia and Flux provide a flexible <a href="./2019-03-05-dp-vs-rl.md">differentiable programming</a> system. Flux does not trade flexibility and abstraction for performance, and in fact strives to achieve both in the same package. For example, Flux is able to target multiple hardware accelerators such as <a href="https://fluxml.ai/Flux.jl/stable/gpu/">GPUs</a> and <a href="https://arxiv.org/pdf/1810.09868.pdf">TPUs</a>. As a result, it is one of the pillars of Julia&#39;s <a href="https://juliahub.com/ui/Packages/CUDAnative/4Zu2W/3.1.0?t&#61;2">deep learning ecosystem</a>, with almost 40 packages leveraging it.</p> <p>Here, we introduce <a href="https://github.com/FluxML/Torch.jl">Torch.jl</a>, a package that wraps optimised kernels from <a href="https://pytorch.org">PyTorch</a>. Even though Julia&#39;s <a href="https://developer.nvidia.com/blog/gpu-computing-julia-programming-language/">GPU compiler</a> is already pretty good for <a href="https://juliahub.com/ui/Packages/CUDAnative/4Zu2W/3.1.0?t&#61;2">general use</a> and under <a href="https://github.com/JuliaGPU/CUDA.jl">heavy development</a>, we provide Torch.jl to leverage well-debugged high performance kernels that have been built by the PyTorch community, much in the same way we use BLAS and LAPACK for Linear Algebra. </p> <p>For popular object detection models - ResNet50, ResNet101 and VGG19 - we compare inference times for Flux using Torch.jl with our native tooling, and find Flux&#43;Torch to be 2-3x faster. On larger batch sizes, the difference is much higher, since Julia&#39;s GPU stack needs further development in the area of memory management and GC. </p> <p float=middle > <img src="../../assets/blogposts/2020-06-29-acclerating-flux-torch/combined_benchmarks_2.png"> <img src="../../assets/blogposts/2020-06-29-acclerating-flux-torch/resnet101.png" height=300 > </p> <p>All runs are with a Tesla K40 &#40;12 GB&#41;, julia v1.4.2, Intel&#40;R&#41; Core&#40;TM&#41; i7-4790 CPU @ 3.60GHz and 32 GB of DDR3 Memory.</p> <p>This project achieves two things. It brings state of the art performance to Julia users who need it today with no fuss, while simulatneously providing benchmarks to identify areas of improvement in the Julia GPU compiler stack, and <a href="https://speed.juliagpu.org/">track them</a>.</p> <h2 id=usage ><a href="#usage" class=header-anchor >Usage</a></h2> <p>Adding Torch.jl is easy. It assumes the presence of a CUDA enabled GPU on the device it is being added to, and assumes a linux system.</p> <h3 id=moving_models_over_to_pytorch_kernels_introducing_torch ><a href="#moving_models_over_to_pytorch_kernels_introducing_torch" class=header-anchor >Moving models over to PyTorch kernels; introducing <code>torch</code></a></h3> <p>Users of Flux are familiar with calling the <code>gpu&#40;model&#41;</code> API to accelerate their models with GPUs. The API for Torch.jl is just as simple - <code>torch&#40;model&#41;</code>.</p> <pre><code class="julia hljs">julia&gt; <span class=hljs-keyword >using</span> Metalhead

julia&gt; resnet = ResNet()

julia&gt; torch_resnet = resnet |&gt; torch</code></pre> <p>Of course this is not just limited to <code>ResNet</code>. Many architectures would benefit from this - such as <code>VGG</code>, <code>DenseNet</code>, <code>Inception</code> etc. Check out <a href="https://github.com/FluxML/Metalhead.jl">Metalhead.jl</a> for some common computer vision models. It also helps improve performance in models such as YOLO via <a href="https://github.com/r3tex/ObjectDetector.jl">ObjectDetector.jl</a>. In addition, large hard to train models like RCNNs also benefit from these kernels.</p> <h3 id=installation ><a href="#installation" class=header-anchor >Installation</a></h3> <pre><code class="julia hljs"><span class=hljs-comment ># Type `]` to enter Pkg mode in the Julia REPL.</span>
pkg&gt; add Torch
[...] <span class=hljs-comment ># Note that this downloads the Torch artifact, which is quite large</span>

julia&gt; <span class=hljs-keyword >using</span> Torch</code></pre> <h2 id=simple_intuitive_api ><a href="#simple_intuitive_api" class=header-anchor >Simple, intuitive API</a></h2> <p>Our APIs make the PyTorch <code>Tensor</code>s mimic Julia arrays closely, in order to provide a Julian experience to Flux users. Torch.jl provides the <code>Tensor</code> type which closely follows the semantics of a regular Julia array, albeit while being managed by PyTorch. One can create a tensor with an API similar to <code>rand</code> or <code>zeros</code>.</p> <pre><code class="julia hljs">julia&gt; z = Tensor(<span class=hljs-number >3</span>,<span class=hljs-number >3</span>)
<span class=hljs-number >3</span>×<span class=hljs-number >3</span> Tensor{<span class=hljs-built_in >Float32</span>,<span class=hljs-number >2</span>} :
 <span class=hljs-number >0.0</span>  <span class=hljs-number >0.0</span>  <span class=hljs-number >0.0</span>
 <span class=hljs-number >0.0</span>  <span class=hljs-number >0.0</span>  <span class=hljs-number >0.0</span>
 <span class=hljs-number >0.0</span>  <span class=hljs-number >0.0</span>  <span class=hljs-number >0.0</span></code></pre> <p>Controlling the device the tensor is loaded on &#40;the default being on CPU&#41; is done via the <code>dev</code> keyword, available in most functions.</p> <pre><code class="julia hljs">julia&gt; z = Tensor(<span class=hljs-number >3</span>,<span class=hljs-number >3</span>, dev = <span class=hljs-number >0</span>)
<span class=hljs-number >3</span>×<span class=hljs-number >3</span> Tensor{<span class=hljs-built_in >Float32</span>,<span class=hljs-number >2</span>} :
 <span class=hljs-number >0.0</span>  <span class=hljs-number >0.0</span>  <span class=hljs-number >0.0</span>
 <span class=hljs-number >0.0</span>  <span class=hljs-number >0.0</span>  <span class=hljs-number >0.0</span>
 <span class=hljs-number >0.0</span>  <span class=hljs-number >0.0</span>  <span class=hljs-number >0.0</span></code></pre> <p>Note that setting <code>dev</code> to <code>-1</code> implies the CPU, and <code>&#91;0,...&#93;</code> represents the ID of the GPU we intend to load the tensor on. The default GPU is assumed to be <code>0</code>. Torch.jl also defines the <code>torch</code> function which behaves like the <code>gpu</code> function already in Flux, moving over structs to Torch instead of CUDA.jl.</p> <pre><code class="julia hljs">julia&gt; <span class=hljs-keyword >using</span> Flux, Metalhead, Torch

julia&gt; <span class=hljs-keyword >using</span> Torch: torch

julia&gt; resnet = ResNet() <span class=hljs-comment ># from Metalhead</span>
ResNet()

julia&gt; tresnet = resnet |&gt; torch
ResNet()</code></pre> <p>We can verify that that we have moved the model parameters to Torch.jl by checking out <code>params</code>.</p> <pre><code class="julia hljs">julia&gt; typeof.(Flux.params(tresnet))
<span class=hljs-number >212</span>-element <span class=hljs-built_in >Array</span>{<span class=hljs-built_in >DataType</span>,<span class=hljs-number >1</span>}:
 Tensor{<span class=hljs-built_in >Float32</span>,<span class=hljs-number >4</span>}
 Tensor{<span class=hljs-built_in >Float32</span>,<span class=hljs-number >1</span>}
[...]</code></pre> <p>It is also possible to move regular Julia arrays back and forth from Torch.jl using the <code>tensor</code> helper function.</p> <pre><code class="julia hljs">julia&gt; r = rand(<span class=hljs-built_in >Float32</span>, <span class=hljs-number >3</span>,<span class=hljs-number >3</span>)
<span class=hljs-number >3</span>×<span class=hljs-number >3</span> <span class=hljs-built_in >Array</span>{<span class=hljs-built_in >Float32</span>,<span class=hljs-number >2</span>}:
 <span class=hljs-number >0.435017</span>  <span class=hljs-number >0.287086</span>  <span class=hljs-number >0.105608</span>
 <span class=hljs-number >0.636305</span>  <span class=hljs-number >0.398222</span>  <span class=hljs-number >0.0682819</span>
 <span class=hljs-number >0.74551</span>   <span class=hljs-number >0.944293</span>  <span class=hljs-number >0.387852</span>

julia&gt; tr = tensor(r, dev = <span class=hljs-number >0</span>) <span class=hljs-comment ># 0 =&gt; GPU:0</span>
<span class=hljs-number >3</span>×<span class=hljs-number >3</span> Tensor {<span class=hljs-built_in >Float32</span>,<span class=hljs-number >2</span>}:
 <span class=hljs-number >0.435017</span>  <span class=hljs-number >0.287086</span>  <span class=hljs-number >0.105608</span>
 <span class=hljs-number >0.636305</span>  <span class=hljs-number >0.398222</span>  <span class=hljs-number >0.0682819</span>
 <span class=hljs-number >0.74551</span>   <span class=hljs-number >0.944293</span>  <span class=hljs-number >0.387852</span>

julia&gt; collect(tr)
<span class=hljs-number >3</span>×<span class=hljs-number >3</span> <span class=hljs-built_in >Array</span>{<span class=hljs-built_in >Float32</span>,<span class=hljs-number >2</span>}:
 <span class=hljs-number >0.435017</span>  <span class=hljs-number >0.287086</span>  <span class=hljs-number >0.105608</span>
 <span class=hljs-number >0.636305</span>  <span class=hljs-number >0.398222</span>  <span class=hljs-number >0.0682819</span>
 <span class=hljs-number >0.74551</span>   <span class=hljs-number >0.944293</span>  <span class=hljs-number >0.387852</span></code></pre> <h2 id=taking_gradients ><a href="#taking_gradients" class=header-anchor >Taking gradients</a></h2> <p>We can use the <a href="https://github.com/Flux/Zygote.jl">Zygote.jl</a> reverse mode AD to differentiate the models using Torch.jl tensors, just like we would with regular Julia <code>Array</code>s.</p> <pre><code class="julia hljs">julia&gt; ip = rand(<span class=hljs-built_in >Float32</span>, <span class=hljs-number >224</span>, <span class=hljs-number >224</span>, <span class=hljs-number >3</span>, <span class=hljs-number >1</span>);

julia&gt; tip = tensor(ip, dev = <span class=hljs-number >0</span>);

julia&gt; gs = gradient(Flux.params(tresnet)) <span class=hljs-keyword >do</span>
         sum(tresnet(tip))
       <span class=hljs-keyword >end</span>;</code></pre> <p>We can now use this gradient to train our models.</p> <h2 id=additional_remarks ><a href="#additional_remarks" class=header-anchor >Additional Remarks</a></h2> <p>In Torch.jl, our aim is also to change as little user code as possible, making it easy to get started with. We invite the community to contribute more kernels and provide features from PyTorch that Julia users might be interested in. For feature requests and issues that you might run into with Torch.jl, please open issues on our <a href="https://github.com/FluxML/Torch.jl/issues">GitHub issue tracker</a>. Contributions via pull requests are highly encouraged&#33;</p> <p>We look forward to seeing folks try it out and let us know on <a href="https://discourse.julialang.org">discourse</a> about their experiences. Cheers&#33;</p> <p class=author >– Dhairya Gandhi, Mike Innes</p> </div> </div> </div> <div class="container footer lighter"> <p>Flux: A Deep Learning Library for the Julia Programming Language </p> <a href="https://twitter.com/FluxML?ref_src=twsrc%5Etfw" class=twitter-follow-button  data-show-count=false >Follow @FluxML</a> <script async src="https://platform.twitter.com/widgets.js" charset=utf-8 ></script> </div> <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script> <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script> <script src="/.//instant.page/1.0.0" type=module  integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>